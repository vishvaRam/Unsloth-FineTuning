{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00ac36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install --upgrade pip\n",
    "# !pip install flashinfer-python\n",
    "# !pip install unsloth\n",
    "# !pip install --no-deps xformers trl peft accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e56b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZNK3c106SymInt6sym_neERKS0_\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb487de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu128\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.8\n",
      "GPU Name: NVIDIA L40S\n",
      "VRAM: 44.5 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd9aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "Model: Qwen/Qwen2.5-3B-Instruct\n",
      "Dataset: Vishva007/RBI-Circular-QA-Dataset\n",
      "Output: ./qwen2.5-3b-rbi-qa\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Model\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    MAX_SEQ_LENGTH = 1536  # Qwen 2.5 supports up to 32K, but 2K is optimal for training\n",
    "    DTYPE = None  # Auto-detect (bfloat16 for modern GPUs)\n",
    "    LOAD_IN_4BIT = True  # Enable 4-bit quantization\n",
    "    \n",
    "    # Dataset\n",
    "    DATASET_NAME = \"Vishva007/RBI-Circular-QA-Dataset\"\n",
    "    DATASET_SPLIT = \"train\"\n",
    "    EVAL_SIZE = 0.1  # 10% for evaluation\n",
    "    SEED = 42\n",
    "    \n",
    "    # LoRA Configuration\n",
    "    LORA_R = 16  # Rank\n",
    "    LORA_ALPHA = 32  # Alpha (2x rank for stronger learning)\n",
    "    LORA_DROPOUT = 0.1  # Dropout for regularization\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "    USE_RSLORA = True  # Enable RS-LoRA for better stability\n",
    "    \n",
    "    # Training\n",
    "    OUTPUT_DIR = \"./qwen2.5-3b-rbi-qa\"\n",
    "    NUM_EPOCHS = 1  # 1 epoch for 47K samples\n",
    "    BATCH_SIZE = 32  # Per device\n",
    "    GRADIENT_ACCUMULATION = 1  # Effective batch size = 32\n",
    "    LEARNING_RATE = 2e-4\n",
    "    WARMUP_RATIO = 0.05  # 5% warmup\n",
    "    LR_SCHEDULER = \"cosine\"\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    EVAL_STEPS = 250\n",
    "    SAVE_STEPS = 250\n",
    "    SAVE_TOTAL_LIMIT = 3\n",
    "    LOGGING_STEPS = 50\n",
    "    \n",
    "    # Early Stopping\n",
    "    EARLY_STOPPING_PATIENCE = 5\n",
    "    EARLY_STOPPING_THRESHOLD = 0.005\n",
    "    \n",
    "    # Output\n",
    "    REPO_ID = \"Vishva007/Qwen2.5-3B-Instruct-RBI-QA\"\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Model: {Config.MODEL_NAME}\")\n",
    "print(f\"Dataset: {Config.DATASET_NAME}\")\n",
    "print(f\"Output: {Config.OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a50ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen 2.5 3B model...\n",
      "==((====))==  Unsloth 2025.11.3: Fast Qwen2 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.521 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f1d3455d4e47a1b18bae484a97d9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7d50dd2e66467f8865a3d19a35b1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19caa86d88524301bc379784c9c34da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5aea82d2994a1a9dd046e5d3d23785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f3b87a43e74bb080f6aa9d1ac62db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c8755c0eba4f5ba06b68c27b3c20dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaae04e140ce475d892f54866d353d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae53da92da44a2d8775091a23862d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded successfully\n",
      "âœ“ Model device: cuda:0\n",
      "âœ“ Model dtype: torch.bfloat16\n",
      "\n",
      "Tokenizer details:\n",
      "  Vocab size: 151665\n",
      "  Pad token: <|vision_pad|>\n",
      "  EOS token: <|im_end|>\n",
      "  BOS token: None\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOAD MODEL & TOKENIZER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading Qwen 2.5 3B model...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=Config.MODEL_NAME,\n",
    "    max_seq_length=Config.MAX_SEQ_LENGTH,\n",
    "    dtype=Config.DTYPE,\n",
    "    load_in_4bit=Config.LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "print(f\"âœ“ Model device: {model.device}\")\n",
    "print(f\"âœ“ Model dtype: {model.dtype}\")\n",
    "\n",
    "# Check tokenizer\n",
    "print(\"\\nTokenizer details:\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"  EOS token: {tokenizer.eos_token}\")\n",
    "print(f\"  BOS token: {tokenizer.bos_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4db2b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LoRA adapters applied\n",
      "  Rank: 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.1\n",
      "  RS-LoRA: True\n",
      "\n",
      "Trainable params: 29,933,568 (1.64%)\n",
      "Total params: 1,830,055,936\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# APPLY LORA ADAPTERS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Applying LoRA configuration...\")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=Config.LORA_R,\n",
    "    target_modules=Config.TARGET_MODULES,\n",
    "    lora_alpha=Config.LORA_ALPHA,\n",
    "    lora_dropout=Config.LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=Config.SEED,\n",
    "    use_rslora=Config.USE_RSLORA,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"âœ“ LoRA adapters applied\")\n",
    "print(f\"  Rank: {Config.LORA_R}\")\n",
    "print(f\"  Alpha: {Config.LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {Config.LORA_DROPOUT}\")\n",
    "print(f\"  RS-LoRA: {Config.USE_RSLORA}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"Total params: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ae5d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Qwen 2.5 chat template...\n",
      "âœ“ Chat template configured\n",
      "\n",
      "Sample formatted prompt:\n",
      "<|im_start|>system\n",
      "You are an expert on RBI regulations.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the RBI's policy on inflation?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SETUP CHAT TEMPLATE FOR QWEN 2.5\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Setting up Qwen 2.5 chat template...\")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",  # Use Qwen 2.5 specific template\n",
    ")\n",
    "\n",
    "print(\"âœ“ Chat template configured\")\n",
    "\n",
    "# Test the template\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert on RBI regulations.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the RBI's policy on inflation?\"},\n",
    "]\n",
    "\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"\\nSample formatted prompt:\")\n",
    "print(formatted[:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d4b9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: Vishva007/RBI-Circular-QA-Dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0256a5a8d73f48e79d23d5f1cc2101d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfb095a68e84485a11786a5b556297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/25.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e21c73225b44e4ba78afd8495d8fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/eval-00000-of-00001.parquet:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07255875fe844e159fa59da4356f4122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/47934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66794a79689d462c93966c039421a19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 47934\n",
      "Columns: ['document', 'filename', 'model_name', 'regulation_area', 'applicable_to', 'issued_on', 'key_topics', 'chunks_text', 'is_table', 'question', 'answer', 'evaluation_criteria', 'category', 'estimated_difficulty', 'rephrased_question', 'rephrased_answer', 'data_source']\n",
      "\n",
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8b20de966946ecaf432d320d27674f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting dataset (num_proc=4):   0%|          | 0/47934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset formatted: 47934 samples\n",
      "\n",
      "======================================================================\n",
      "SAMPLE FORMATTED EXAMPLE:\n",
      "======================================================================\n",
      "<|im_start|>system\n",
      "You are a highly knowledgeable AI assistant with expertise in Indian banking and \n",
      "    financial regulations, particularly those outlined in Reserve Bank of India (RBI) circulars. \n",
      "    Your task is to answer questions based on the RBI circulars and related financial regulations.\n",
      "    Provide accurate, specific answers including relevant dates, amounts, and institutional details.<|im_end|>\n",
      "<|im_start|>user\n",
      "What relaxations were provided by the Reserve Bank of India regarding the ...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOAD & FORMAT DATASET\n",
    "# ==============================================================================\n",
    "\n",
    "def format_rbi_dataset(examples):\n",
    "    \"\"\"\n",
    "    Format RBI QA dataset for Qwen 2.5 chat template\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    system_msg = \"\"\"You are a highly knowledgeable AI assistant with expertise in Indian banking and \n",
    "    financial regulations, particularly those outlined in Reserve Bank of India (RBI) circulars. \n",
    "    Your task is to answer questions based on the RBI circulars and related financial regulations.\n",
    "    Provide accurate, specific answers including relevant dates, amounts, and institutional details.\"\"\"\n",
    "    \n",
    "    for i in range(len(examples['question'])):\n",
    "        # Create chat messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": examples['question'][i]},\n",
    "            {\"role\": \"assistant\", \"content\": examples['answer'][i]}\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False  # Include assistant response\n",
    "        )\n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset: {Config.DATASET_NAME}\")\n",
    "dataset = load_dataset(Config.DATASET_NAME, split=Config.DATASET_SPLIT)\n",
    "\n",
    "print(f\"Total records: {len(dataset)}\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# Apply formatting\n",
    "print(\"\\nFormatting dataset...\")\n",
    "dataset = dataset.map(\n",
    "    format_rbi_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=4,  # Parallel processing\n",
    "    desc=\"Formatting dataset\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Dataset formatted: {len(dataset)} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE FORMATTED EXAMPLE:\")\n",
    "print(\"=\"*70)\n",
    "print(dataset[0]['text'][:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23764833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train/eval...\n",
      "Train samples: 43140\n",
      "Eval samples: 4794\n",
      "\n",
      "Training details:\n",
      "  Steps per epoch: 1348\n",
      "  Total steps: 1348\n",
      "  Eval every: 250 steps\n",
      "  Save every: 250 steps\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN/EVAL SPLIT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Splitting dataset into train/eval...\")\n",
    "\n",
    "dataset_splits = dataset.train_test_split(\n",
    "    test_size=Config.EVAL_SIZE,\n",
    "    seed=Config.SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(dataset_splits['train'])}\")\n",
    "print(f\"Eval samples: {len(dataset_splits['test'])}\")\n",
    "\n",
    "# Calculate training steps\n",
    "steps_per_epoch = len(dataset_splits['train']) // (Config.BATCH_SIZE * Config.GRADIENT_ACCUMULATION)\n",
    "total_steps = steps_per_epoch * Config.NUM_EPOCHS\n",
    "\n",
    "print(\"\\nTraining details:\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Eval every: {Config.EVAL_STEPS} steps\")\n",
    "print(f\"  Save every: {Config.SAVE_STEPS} steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a849310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured!\n",
      "  Effective batch size: 32\n",
      "  Learning rate: 0.0002\n",
      "  Scheduler: cosine\n",
      "  Precision: BF16\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TRAINING ARGUMENTS\n",
    "# ==============================================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=Config.OUTPUT_DIR,\n",
    "    run_name=\"qwen2.5-3b-rbi-qa\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=Config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=Config.LEARNING_RATE,\n",
    "    lr_scheduler_type=Config.LR_SCHEDULER,\n",
    "    warmup_ratio=Config.WARMUP_RATIO,\n",
    "    weight_decay=Config.WEIGHT_DECAY,\n",
    "    max_grad_norm=Config.MAX_GRAD_NORM,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Memory & Performance\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=Config.EVAL_STEPS,\n",
    "    per_device_eval_batch_size=Config.BATCH_SIZE,\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=Config.SAVE_STEPS,\n",
    "    save_total_limit=Config.SAVE_TOTAL_LIMIT,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=Config.LOGGING_STEPS,\n",
    "    logging_strategy=\"steps\",\n",
    "    report_to=\"tensorboard\",\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=Config.SEED,\n",
    "    data_seed=Config.SEED,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"  Effective batch size: {Config.BATCH_SIZE * Config.GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Learning rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"  Scheduler: {Config.LR_SCHEDULER}\")\n",
    "print(\"  Precision: BF16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51ccfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up SFTTrainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a05e7de761a4eb2987b6d4529c5271a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=116):   0%|          | 0/43140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3e2cec7f9b4201bf75a9ed9b2a110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=116):   0%|          | 0/4794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Trainer configured\n",
      "  Model: Qwen 2.5 3B with LoRA\n",
      "  Train samples: 43140\n",
      "  Eval samples: 4794\n",
      "  Early stopping patience: 5\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SETUP TRAINER\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Setting up SFTTrainer...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_splits['train'],\n",
    "    eval_dataset=dataset_splits['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=Config.MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,  # Disable packing for better quality\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=Config.EARLY_STOPPING_PATIENCE,\n",
    "    early_stopping_threshold=Config.EARLY_STOPPING_THRESHOLD,\n",
    ")\n",
    "trainer.add_callback(early_stopping)\n",
    "\n",
    "print(\"âœ“ Trainer configured\")\n",
    "print(\"  Model: Qwen 2.5 3B with LoRA\")\n",
    "print(f\"  Train samples: {len(dataset_splits['train'])}\")\n",
    "print(f\"  Eval samples: {len(dataset_splits['test'])}\")\n",
    "print(f\"  Early stopping patience: {Config.EARLY_STOPPING_PATIENCE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "935ec43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING TRAINING - QWEN 2.5 3B ON RBI QA DATASET\n",
      "======================================================================\n",
      "Start time: 2025-11-24 16:21:13\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 43,140 | Num Epochs = 1 | Total steps = 1,349\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 29,933,568 of 3,115,872,256 (0.96% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1349' max='1349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1349/1349 52:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.791800</td>\n",
       "      <td>0.781934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>0.688187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.629984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>0.592467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.572400</td>\n",
       "      <td>0.579091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "======================================================================\n",
      "End time: 2025-11-24 17:14:01\n",
      "Total time: 3167.23 seconds\n",
      "Samples/sec: 13.62\n",
      "Final train loss: 0.708442140809689\n",
      "======================================================================\n",
      "\n",
      "Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 02:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval loss: 0.5791\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# START TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING - QWEN 2.5 3B ON RBI QA DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Train\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Samples/sec: {trainer_stats.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final train loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get final evaluation\n",
    "print(\"\\nRunning final evaluation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Final eval loss: {eval_results['eval_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5af300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING MODEL\n",
      "======================================================================\n",
      "Saving to: ./qwen2.5-3b-rbi-qa/merged-16bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac90d225f547a48df16de3785e7969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e485b1270140309bf1f6e4caadf77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f253eac631f040d79e09dd4cc5b013a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0388f90469e142d188c5623e69cc2378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/qwen2.5-3b-rbi-qa/merged-16bit`\n",
      "âœ“ Model saved to ./qwen2.5-3b-rbi-qa/merged-16bit\n",
      "  Format: Merged 16-bit (ready for vLLM/SGLang)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SAVE MODEL - MERGED 16-BIT FOR INFERENCE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_path = f\"{Config.OUTPUT_DIR}/merged-16bit\"\n",
    "print(f\"Saving to: {output_path}\")\n",
    "\n",
    "# Save as merged 16-bit model (best for inference/vLLM/SGLang)\n",
    "model.save_pretrained_merged(\n",
    "    output_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model saved to {output_path}\")\n",
    "print(\"  Format: Merged 16-bit (ready for vLLM/SGLang)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4062d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91a575ae56c4e79920d38c8595ea496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pushing to: Vishva007/Qwen2.5-3B-Instruct-RBI-QA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf1e73521be42319a1ee383669a7b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df237046b63c4af2890d47e987ad390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db8be51233644dab23a104cb3ce2f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e0ad8bb1984d96b06455499cb5da57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.22s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c63f0550ac9452c8efd3efaba07746c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94879432b9134cccae1d7c3e70c18dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded9c4d9b90542b3a412aab6cbc3154c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:49<00:49, 49.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43076d68c57841b1be0417e699129223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476a56411a9d4435abe60608ca808db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:21<00:00, 40.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/workspace/Vishva007/Qwen2.5-3B-Instruct-RBI-QA`\n",
      "âœ“ Model pushed to Vishva007/Qwen2.5-3B-Instruct-RBI-QA\n",
      "ðŸ”— View at: https://huggingface.co/Vishva007/Qwen2.5-3B-Instruct-RBI-QA\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PUSH TO HUGGING FACE HUB (Optional)\n",
    "# ==============================================================================\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "       \n",
    "# Login\n",
    "notebook_login()\n",
    "    \n",
    "print(f\"\\nPushing to: {Config.REPO_ID}\")\n",
    "    \n",
    "# Push merged model\n",
    "model.push_to_hub_merged(\n",
    "        Config.REPO_ID,\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "        token=True,  # Use saved token\n",
    "    )\n",
    "    \n",
    "print(f\"âœ“ Model pushed to {Config.REPO_ID}\")\n",
    "print(f\"ðŸ”— View at: https://huggingface.co/{Config.REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c92bf-b116-483b-ba83-96d105fc6607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
